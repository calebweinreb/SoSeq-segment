{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Background segmentation\n",
    "\n",
    "![title](../scratch/pipeline_diagram-2.png)\n",
    "\n",
    "The purpose of this notebook is to train a neural network (UNET) for segmenting animals from the background.\n",
    "The steps include:\n",
    "\n",
    "* 2.1 Building an initial training set\n",
    "* 2.2 Train the neural network (UNET)\n",
    "* 2.3 Review results and possibly augment training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.io import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Build an initial training set\n",
    "\n",
    "Scan through all recordings and save a subset of frames for manual annotation. It is assumed that the recordings were generated using the SoSeq-acquisition repository or converted to the correct format using the SoSeq-acquisition repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../../SoSeq-acquire/data_cropped/' # path to directory containing video recordings\n",
    "                               # it is assumed that the recordings are all named by the convention:\n",
    "                               # <data_directory>/<name>_color.mp4\n",
    "                               # <data_directory>/<name>_depth.avi\n",
    "            \n",
    "frame_size = (540,480) # the dimensions of the video files\n",
    "            \n",
    "save_directory_train = data_directory + '/segmentation_train' # training images and annotations will be saved here\n",
    "save_directory_test = data_directory + '/segmentation_test' # test images and annotations will be saved here\n",
    "            \n",
    "number_test_frames = 100 # number of test images to annotate - this test set will generally be fixed over time\n",
    "number_train_frames = 200 # number of train images to annotate - this train set may be augmented over time\n",
    "\n",
    "# it is faster to load a chunk of consecutive frames than to load each frame separately\n",
    "buffer_size = 100 # the number of consecutive frames to load at once\n",
    "buffer_skip = 20 # the gap between frames to be saved in the train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load video lengths and set up save directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the length of each video\n",
    "video_lengths = get_video_lengths(data_directory)\n",
    "\n",
    "# create the training/testing directories\n",
    "for directory in [save_directory_train,save_directory_test]:\n",
    "    if not os.path.exists(directory): os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly sample frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 195 out of 200 train frames\n",
      "Saved 100 out of 100 test frames"
     ]
    }
   ],
   "source": [
    "videos = list(video_lengths.keys())\n",
    "num_saved = 0\n",
    "while num_saved < number_train_frames+number_test_frames:\n",
    "    video = np.random.choice(videos)\n",
    "    frame = np.random.randint(0,video_lengths[video])-buffer_size\n",
    "    t = time.time()\n",
    "    frame_buffer = read_color_frames(data_directory+'/'+video, range(frame,frame+buffer_size), frame_size=frame_size).squeeze()\n",
    "    \n",
    "    for i in range(0,buffer_size, buffer_skip):\n",
    "        save_dir = (save_directory_train if num_saved < number_train_frames else save_directory_test)\n",
    "        cv2.imwrite(save_dir+'/'+video+'_'+str(frame+i)+'.png',frame_buffer[i,:,:,::-1])\n",
    "        num_saved += 1\n",
    "    \n",
    "    if num_saved < number_train_frames: print('\\rSaved',num_saved,'out of',number_train_frames,'train frames',end='')\n",
    "    elif num_saved == number_train_frames: print('')\n",
    "    else: print('\\rSaved',num_saved-number_train_frames,'out of',number_test_frames,'test frames',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "save_directory_test": "../../SoSeq-acquire/data_cropped//segmentation_test",
     "save_directory_train": "../../SoSeq-acquire/data_cropped//segmentation_train"
    }
   },
   "source": [
    "### Annotate frames\n",
    "\n",
    "Frames can be annotated using labelme, as described below. If you are using a compyter with a graphical interface, then labelme can be started using by running ```labelme``` in the soseq environment. Otherwise, copy the train and test images to your local computer and run labelme from there. Instructions for installing labelme can be found on the [labelme githib page](https://github.com/wkentaro/labelme). The train and test images that have just been saved are in the following directories:\n",
    "\n",
    "```\n",
    "{{save_directory_train}}\n",
    "{{save_directory_test}}\n",
    "```\n",
    "\n",
    "Once you have opened labelme, load the train/test images directory\n",
    "\n",
    "![title](../scratch/labelme1-01.png)\n",
    "\n",
    "Click \"Create Polygons\" and then click a circumference around one of the animals, finishing on the initial vertex\n",
    "\n",
    "![title](../scratch/labelme2-01.png)\n",
    "\n",
    "Upon completing the polygon, enter a name such as species name. Make sure to use the same label for each image and animal instance. You will be asked to input the name during network training. \n",
    "\n",
    "![title](../scratch/labelme3-01.png)\n",
    "\n",
    "Outline each of the animals in frame and then click \"Save\" or use the save keyboard shortcut (command-shift-S on a mac). Labelme will automatically suggest a directory for saving. Do not change this directory. \n",
    "\n",
    "![title](../scratch/labelme4-01.png)\n",
    "\n",
    "When two animals overlap, the outline of the occluded animal should only include its visible parts. Do not try to predict the outline of the hiden portions and avoid overlap between the polygons drawn for each animal. If one animal is fully bisected by the occlusion from another, then draw a separate polygon for each of the visible portions. \n",
    "\n",
    "![title](../scratch/labelme5-01.png)\n",
    "\n",
    "When you have annotated all train and test frames, the image directories should contain a .json file for each original .png file. If annotations were performed on a separate computer, copy them back to the original image directories:\n",
    "\n",
    "```\n",
    "{{save_directory_train}}\n",
    "{{save_directory_test}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train the neural network (UNET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate all annotations into a pair of h5 files then build and train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_name = 'mouse' # this string should match the name you used for the polygons in labelme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.draw import polygon\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def load_annotation(annotation_path, frame_size):\n",
    "    segmentations = []\n",
    "    labels = []\n",
    "    for ii,shape in enumerate(json.load(open(annotation_path))['shapes']):\n",
    "        seg = np.zeros((np.max(frame_size),np.max(frame_size)))\n",
    "        poly = np.array(shape['points'])\n",
    "        rr,cc = polygon(poly[:,0], poly[:,1], (np.max(frame_size),np.max(frame_size)))\n",
    "        seg[cc,rr] = 255\n",
    "        segmentations.append(seg[:frame_size[1],:frame_size[0],None])\n",
    "        labels.append(shape['label'])\n",
    "    segmentations = np.concatenate(segmentations, axis=2)\n",
    "    return segmentations.astype(np.uint8), labels\n",
    "\n",
    "\n",
    "def aggregate_annotations_background_segmentation(annotations_directory, frame_size, animal_name):\n",
    "    dataset = h5py.File(annotations_directory+'/dataset.h5','w')\n",
    "    for f in os.listdir(annotations_directory):\n",
    "        if '.json' in f:\n",
    "            segmentation,labels = load_annotation(f, frame_size)\n",
    "            use_regions = np.array([label==animal_name for label in labels])\n",
    "            if use_regions.sum() > 0:\n",
    "                mask = np.any(segmentation[:,:,use_regions],axis=2)\n",
    "                \n",
    "                video_prefix = data_directory+'/'+f.split('color.mp4')[0]\n",
    "                frame = int(f.split('_')[-1].split('.png')[0])\n",
    "                color = read_color_frames(video_prefix+'color.mp4', [frame], frame_size=frame_size).squeeze()\n",
    "                depth = read_depth_frames16(video_prefix+'depth.avi', [frame], frame_size=frame_size).squeeze()\n",
    "                \n",
    "                \n",
    "\n",
    "#    19_11_20-C57_GRIN5-MR-000371392012_color.mp4_3861.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np, os\n",
    "import UNET.u_net as unet\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt, h5py\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.get_unet_512(num_classes=1, input_shape=(512, 512, 4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function for the training data\n",
    "def train_process(data):\n",
    "    img, mask = data\n",
    "    img = cv2.resize(img, SIZE).astype(float)\n",
    "    mask = cv2.resize(mask, SIZE).astype(float)\n",
    "    img = img/255\n",
    "    \n",
    "    ff = img[:,:,3].flatten()>0\n",
    "    for i in range(4):\n",
    "        img[:,:,i] = img[:,:,i] / img[:,:,i].flatten()[ff].mean()\n",
    "\n",
    "    mask = np.expand_dims(mask, axis=2)/255\n",
    "    return (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function for the validation data, no data augmentation\n",
    "def validation_process(data):\n",
    "    img, mask = data\n",
    "    img = cv2.resize(img, SIZE).astype(float)\n",
    "    mask = cv2.resize(mask, SIZE).astype(float)\n",
    "    img = img/255.\n",
    "    \n",
    "    ff = img[:,:,3].flatten()>0\n",
    "    for i in range(4):\n",
    "        img[:,:,i] = img[:,:,i] / img[:,:,i].flatten()[ff].mean()\n",
    "    \n",
    "    mask = np.expand_dims(mask, axis=2)/255\n",
    "    return (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "training_dir = 'training_images_UNET/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_maker(training_paths):\n",
    "    while True:\n",
    "        ix = np.random.randint(500,len(training_paths),BATCH_SIZE)\n",
    "        out_img = []\n",
    "        out_mask = []\n",
    "        for i in ix:\n",
    "            data = np.load(training_paths[i])['arr_0'].item()\n",
    "            img = data['image'].astype(float)\n",
    "            mask = data['masks'].squeeze().astype(np.uint8)*255\n",
    "            img,mask = train_process((img,mask))\n",
    "            out_img.append(img); out_mask.append(mask)\n",
    "        yield np.array(out_img), np.array(out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_generator_maker(training_paths):\n",
    "    while True:\n",
    "        ix = np.random.randint(0,500,BATCH_SIZE)\n",
    "        out_img = []\n",
    "        out_mask = []\n",
    "        for i in ix:\n",
    "            data = np.load(training_paths[i])['arr_0'].item()\n",
    "            img = data['image'].astype(float)\n",
    "            mask = data['masks'].squeeze().astype(np.uint8)*255\n",
    "            img,mask = train_process((img,mask))\n",
    "            out_img.append(img); out_mask.append(mask)\n",
    "        yield np.array(out_img), np.array(out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='UNET/weights/best_weights.hdf5',\n",
    "                             save_best_only=False,\n",
    "                             save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=150\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=2000,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (soseq)",
   "language": "python",
   "name": "soseq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
